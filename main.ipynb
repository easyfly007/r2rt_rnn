{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model:\n",
      "- learns neither dependency: 0.661563238158\n",
      "- learns first dependency:   0.519166699707\n",
      "- learns both dependencies:  0.454454367449\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Expected cross entropy loss if the model:\")\n",
    "print(\"- learns neither dependency:\", -(0.625 * np.log(0.625) +\n",
    "                                      0.375 * np.log(0.375)))\n",
    "# Learns first dependency only ==> 0.51916669970720941\n",
    "print(\"- learns first dependency:  \",\n",
    "      -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "      -0.5 * (0.625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "print(\"- learns both dependencies: \", -0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25))\n",
    "      - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L95\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 250 steps: 0.661281494498\n",
      "Average loss at step 200 for last 250 steps: 0.602054635286\n",
      "Average loss at step 300 for last 250 steps: 0.588427830935\n",
      "Average loss at step 400 for last 250 steps: 0.577404659986\n",
      "Average loss at step 500 for last 250 steps: 0.530450355709\n",
      "Average loss at step 600 for last 250 steps: 0.519598836601\n",
      "Average loss at step 700 for last 250 steps: 0.522203704715\n",
      "Average loss at step 800 for last 250 steps: 0.519987429976\n",
      "Average loss at step 900 for last 250 steps: 0.524731419086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1caa4831898>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkZRT0YuIpJyKXkQk5VT0IiIp9/8BS/CTDv1ARXoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1caa1297be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(1,num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow.nn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1a2792ffb645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasic_rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow.nn'"
     ]
    }
   ],
   "source": [
    "from tensorflow.nn import basic_rnn\n",
    "def plot_learning_curve(num_steps, state_size=4, epochs=1):\n",
    "    global losses, total_loss, final_state, train_step, x, y, init_state\n",
    "    tf.reset_default_graph()\n",
    "    g = tf.get_default_graph()\n",
    "    losses, total_loss, final_state, train_step, x, y, init_state = \\\n",
    "        basic_rnn.setup_graph(g,\n",
    "            basic_rnn.RNN_config(num_steps=num_steps, state_size=state_size))\n",
    "    res = train_network(epochs, num_steps, state_size=state_size, verbose=False)\n",
    "    plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_learning_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0bcfd4db3966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNUM_STEPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_learning_curve' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUM_STEPS = 1\n",
    "\"\"\"\n",
    "plot_learning_curve(num_steps=1, state_size=4, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_learning_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9b40377dd089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNUM_STEPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_learning_curve' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUM_STEPS = 10\n",
    "\"\"\"\n",
    "plot_learning_curve(num_steps=10, state_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable softmax/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"c:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n  File \"c:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d1976003d2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrnn_output\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrnn_outputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable softmax/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"c:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n  File \"c:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "Inputs\n",
    "\"\"\"\n",
    "\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'rnn/basic_rnn_cell'; and the cell was not constructed as BasicRNNCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5f1942b65aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mrnn_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \"\"\"\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    718\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2621\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m     \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2454\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2456\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2457\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2404\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2406\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    703\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    704\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[1;31m# Pack state if using state tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[1;34m\"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0m_checked_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"basic_rnn_cell\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reuse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m       output = self._activation(\n\u001b[1;32m    122\u001b[0m           _linear([inputs, state], self._num_units, True))\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\echok\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m_checked_scope\u001b[0;34m(cell, scope, reuse, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[1;34m\"To share the weights of an RNNCell, simply \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[1;34m\"reuse it in your second calculation, or create a new one with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[1;31m# Everything is OK.  Update the cell's scope and yield it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'rnn/basic_rnn_cell'; and the cell was not constructed as BasicRNNCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "Inputs\n",
    "\"\"\"\n",
    "\n",
    "rnn_inputs = tf.one_hot(x, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = tf.reshape(\n",
    "            tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b,\n",
    "            [batch_size, num_steps, num_classes])\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
